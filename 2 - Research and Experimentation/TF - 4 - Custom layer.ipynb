{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Build custom layers\n",
    "\n",
    "# TensorFlow provides both a set of many common layers as a well as easy ways for you to write\n",
    "# your own application-specific layers either from scratch or as the composition of existing layers.\n",
    "\n",
    "# It is recommanded to use tf.keras to build a NN. (TensorFlow includes the full Keras API in the tf.keras package)\n",
    "# But let's understand how to create a layer in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0903 11:13:06.097899 140735803462528 __init__.py:690] \n",
      "\n",
      "  TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0.\n",
      "\n",
      "  Please upgrade your code to TensorFlow 2.0:\n",
      "    * https://www.tensorflow.org/beta/guide/migration_guide\n",
      "\n",
      "  Or install the latest stable TensorFlow 1.X release:\n",
      "    * `pip install -U \"tensorflow==1.*\"`\n",
      "\n",
      "  Otherwise your code may be broken by the change.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=29, shape=(10, 10), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of how to create a layer with keras\n",
    "\n",
    "# Most layers take as a first argument the number of output dimensions / channels.\n",
    "layer = tf.keras.layers.Dense(100)\n",
    "\n",
    "# # The number of input dimensions is often unnecessary, as it can be inferred\n",
    "# the first time the layer is used, but it can be provided if you want to\n",
    "# specify it manually, which is useful in some complex models. \n",
    "# It is necessary to mention it to use the model.summary() method\n",
    "layer = tf.keras.layers.Dense(10, input_shape=(None, 5))\n",
    "\n",
    "# None means this dimension is variable.\n",
    "# The first dimension in a keras model is always the batch size.\n",
    "\n",
    "# To use a layer, simply call it. Specify the input as argument\n",
    "\n",
    "layer(tf.zeros((10, 5))) # shape is 10 x 10 (batch size x number of hidden units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_1/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
       " array([[ 0.44145542, -0.5323926 , -0.104366  ,  0.19480467,  0.1309818 ,\n",
       "          0.24342811,  0.2775995 ,  0.37767726, -0.23948544, -0.5483116 ],\n",
       "        [ 0.19261807,  0.5645605 , -0.00918198, -0.09430081, -0.16263017,\n",
       "          0.3170185 , -0.27144808,  0.26175082,  0.24306595,  0.14648199],\n",
       "        [-0.58918357, -0.3689554 ,  0.16178197,  0.40211958, -0.6148025 ,\n",
       "          0.5537335 ,  0.5508148 ,  0.60364324, -0.35310385,  0.5201954 ],\n",
       "        [-0.4334734 , -0.38740355,  0.23373508, -0.39353704, -0.5196559 ,\n",
       "         -0.6318864 ,  0.39657396,  0.13487875, -0.56456244,  0.59374934],\n",
       "        [-0.48353451,  0.16812855, -0.43188772,  0.2932359 , -0.2356584 ,\n",
       "          0.28270495, -0.04375225,  0.4093544 , -0.28553525, -0.23817027]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Layers have many useful methods. For example, you can inspect all variables\n",
    "# in a layer using layer.variables and trainable variables using\n",
    "# layer.trainable_variables. In this case a fully-connected layer\n",
    "# will have variables for weights and biases.\n",
    "layer.variables # W (10 x 5) and b (10 x 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'dense_1/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
       " array([[ 0.44145542, -0.5323926 , -0.104366  ,  0.19480467,  0.1309818 ,\n",
       "          0.24342811,  0.2775995 ,  0.37767726, -0.23948544, -0.5483116 ],\n",
       "        [ 0.19261807,  0.5645605 , -0.00918198, -0.09430081, -0.16263017,\n",
       "          0.3170185 , -0.27144808,  0.26175082,  0.24306595,  0.14648199],\n",
       "        [-0.58918357, -0.3689554 ,  0.16178197,  0.40211958, -0.6148025 ,\n",
       "          0.5537335 ,  0.5508148 ,  0.60364324, -0.35310385,  0.5201954 ],\n",
       "        [-0.4334734 , -0.38740355,  0.23373508, -0.39353704, -0.5196559 ,\n",
       "         -0.6318864 ,  0.39657396,  0.13487875, -0.56456244,  0.59374934],\n",
       "        [-0.48353451,  0.16812855, -0.43188772,  0.2932359 , -0.2356584 ,\n",
       "          0.28270495, -0.04375225,  0.4093544 , -0.28553525, -0.23817027]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The variables are also accessible through nice accessors\n",
    "layer.kernel, layer.bias # kernel reference to W and bias to b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0903 11:13:06.275596 140735803462528 deprecation.py:323] From <ipython-input-7-ee93dd541baf>:22: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward propagation of the customer layer : \n",
      " tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n",
      "\n",
      "\n",
      "Trainable parameters:  [<tf.Variable 'my_dense_layer/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
      "array([[ 0.3174814 , -0.34096202,  0.60105854,  0.24612862, -0.33708492,\n",
      "         0.49505514, -0.16772625,  0.43777126, -0.2138356 ,  0.537986  ],\n",
      "       [-0.52330047, -0.16212142, -0.17449832,  0.19706953, -0.5946049 ,\n",
      "        -0.29093936,  0.02717417,  0.4796396 , -0.44050065,  0.20151687],\n",
      "       [ 0.34372354, -0.57321155, -0.17838523, -0.54831445, -0.2651137 ,\n",
      "        -0.6297469 , -0.00578636, -0.44437638,  0.43090492, -0.13636315],\n",
      "       [-0.51094544,  0.30369794,  0.54648536, -0.37531465, -0.34663862,\n",
      "        -0.24795827,  0.39757282, -0.46719846, -0.5896617 , -0.07465887],\n",
      "       [-0.44109702,  0.0174427 , -0.51436484, -0.27730653,  0.00702298,\n",
      "        -0.11669827, -0.30986327, -0.47179693, -0.05508751,  0.14382106]],\n",
      "      dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "### Implementing custom layer\n",
    "\n",
    "# Create a layer by inheriting from tf.keras.layers.Layers\n",
    "\n",
    "class myDenseLayer(tf.keras.layers.Layer):\n",
    "    '''  Define a custom layer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_outputs):\n",
    "        '''  Do all input-independent initialization\n",
    "        '''\n",
    "        super(myDenseLayer, self).__init__() # Pass no argument to the parent class\n",
    "        self.num_outputs = num_outputs\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        ''' Do the rest of the initialization since you know the shape of the input\n",
    "            input_shape          Shape of the input (TensorShape)\n",
    "        '''\n",
    "        # Add weight to the model (Adds a new variable to the layer.)\n",
    "        self.kernel = self.add_variable('kernel', # name\n",
    "                                         shape=[input_shape[-1], # number of features\n",
    "                                                self.num_outputs]) # number of hidden units\n",
    "    def call(self, input):\n",
    "        ''' Do forward propagation\n",
    "            input          Tensor\n",
    "        '''\n",
    "        return tf.matmul(input, self.kernel) # (m x n) * (n x num_ouputs) -> (m x num_outputs)\n",
    "    \n",
    "# Initialize and test our custom layer\n",
    "layer = myDenseLayer(10)\n",
    "print('Forward propagation of the customer layer : \\n', layer(tf.zeros((10, 5))))\n",
    "print('\\n\\nTrainable parameters: ', layer.trainable_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]], shape=(1, 2, 3, 3), dtype=float32)\n",
      "['res_net_layer_1/conv2d_2/kernel:0', 'res_net_layer_1/conv2d_2/bias:0', 'res_net_layer_1/batch_normalization_2/gamma:0', 'res_net_layer_1/batch_normalization_2/beta:0', 'res_net_layer_1/conv2d_3/kernel:0', 'res_net_layer_1/conv2d_3/bias:0', 'res_net_layer_1/batch_normalization_3/gamma:0', 'res_net_layer_1/batch_normalization_3/beta:0', 'res_net_layer_1/conv2d_4/kernel:0', 'res_net_layer_1/conv2d_4/bias:0', 'res_net_layer_1/batch_normalization_4/gamma:0', 'res_net_layer_1/batch_normalization_4/beta:0']\n"
     ]
    }
   ],
   "source": [
    "### Composing layers\n",
    "\n",
    "# To create a layer which combine several layers, inherit from keras.Model\n",
    "# Let's build a ResNet layer\n",
    "\n",
    "class ResNet_layer(tf.keras.Model):\n",
    "    ''' Construct a ResNet layer: \n",
    "        X is fed to (CONV -> BatchNorm -> ReLu) (x2) -> CONV -> BatchNorm -> + X -> Relu\n",
    "        ('Skip connection' over 2 layers)\n",
    "        The shape of the filters in the first and last convolution is (1 x 1)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, kernel_size, filters):\n",
    "        ''' Do all input-independent initialization\n",
    "            \n",
    "            kernel_size      size of the filter for the second convolution\n",
    "            filters          the number of filters in the convolution (list of integer)\n",
    "        '''\n",
    "        super(ResNet_layer, self).__init__(name = '')\n",
    "        filter_1, filter_2, filter_3 = filters # Unpack the number of filters in each convolution\n",
    "        \n",
    "        # First layer\n",
    "        self.conv_1 = tf.keras.layers.Conv2D(filter_1, (1, 1))\n",
    "        self.bn_1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # Second layer\n",
    "        self.conv_2 = tf.keras.layers.Conv2D(filter_2, kernel_size, padding = 'same')\n",
    "        self.bn_2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # Third layer\n",
    "        self.conv_3 = tf.keras.layers.Conv2D(filter_3, (1, 1))\n",
    "        self.bn_3 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "    def call(self, input_tensor, training = False):\n",
    "        ''' Given the input, perform a forward-propagation step\n",
    "            \n",
    "            training    Whether to return the output in training mode (normalized with statistics of the current batch)\n",
    "                        or in inference mode (normalized with moving statistics)\n",
    "        '''\n",
    "        \n",
    "        # First layer\n",
    "        X = self.conv_1(input_tensor)\n",
    "        X = self.bn_1(X, training = training) \n",
    "        X = tf.nn.relu(X)\n",
    "        \n",
    "        # Second layer\n",
    "        X = self.conv_2(input_tensor)\n",
    "        X = self.bn_2(X, training = training)\n",
    "        X = tf.nn.relu(X)\n",
    "        \n",
    "        # Third layer\n",
    "        X = self.conv_3(input_tensor)\n",
    "        X = self.bn_3(X, training = training)\n",
    "        # Add the input ('Skip connection' link)\n",
    "        X = X + input_tensor\n",
    "        X = tf.nn.relu(X)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "# Create a ResNet layer\n",
    "layer = ResNet_layer(1, [1, 2, 3])\n",
    "# Perform forward propagation \n",
    "print(layer(tf.zeros((1, 2, 3, 3))))\n",
    "# Print each parameters\n",
    "print([var.name for var in layer.trainable_variables])\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=505, shape=(1, 2, 3, 3), dtype=float32, numpy=\n",
       "array([[[[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Much of the time, however, models which compose many layers simply call one layer after the other.\n",
    "# This can be done in very little code using tf.keras.Sequential\n",
    "\n",
    "customer_layer = tf.keras.Sequential([tf.keras.layers.Conv2D(1, (1, 1)),\n",
    "                                     tf.keras.layers.BatchNormalization(), # No ReLU ?\n",
    "                                     tf.keras.layers.Conv2D(2, (1, 1), padding = 'same'),\n",
    "                                     tf.keras.layers.BatchNormalization(),\n",
    "                                     tf.keras.layers.Conv2D(3, (1, 1)),\n",
    "                                     tf.keras.layers.BatchNormalization()])\n",
    "\n",
    "customer_layer(tf.zeros((1, 2, 3, 3))) # 1 training example, n_h = 2, n_w = 3, n_c = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
