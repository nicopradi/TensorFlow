{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementation of AlexNet with TF 2.0\n",
    "\n",
    "# Reference: https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n",
    "\n",
    "# Remarks: The original implementation make use of 2 GPU. \n",
    "#          Due to the lack of ressources, this implementation is designed to work on a single CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-rc0\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)\n",
    "\n",
    "#TODO: rewrite a version in TF2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Pre-process the data (data augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Create LRN customer layer in Keras\n",
    "\n",
    "class LRN_layer(tf.keras.layers.Layer):\n",
    "    ''' Create a layer for computing the Local Response normalization.\n",
    "        The activations are averaged over adjacent channels at the same spatial position.\n",
    "        This layer is described in more detail in AlexNet paper.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, k = 2, n = 5, alpha = 10**(-4), beta = 0.75, **kwargs):\n",
    "        #self.output_dim = output_dim\n",
    "        self.k = k\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        super(LRN_layer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # This layer has no trainable weights\n",
    "        super(LRN_layer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        ''' Apply the transformation on x\n",
    "        '''\n",
    "        # Initialize a list of tensor to stack the result of each batch\n",
    "        output_list = []\n",
    "        # Compute the normalization factor for each activation function\n",
    "        for c in range(n_c):\n",
    "            # Compute the lower and upper limit of the average \n",
    "            begin = max(0, c - int(self.n/2.0))\n",
    "            end = min(n_c - 1, c + int(self.n/2.0))\n",
    "            for h in range(n_h):\n",
    "                for w in range(n_w):\n",
    "                    output_list.append(tf.reduce_sum(tf.square(x[:, begin:end, h, w]), axis = 1))\n",
    "                    \n",
    "        # Create a tensor containing the sum in the denominator for each batch\n",
    "        sum_adjacent = tf.stack(output_list)\n",
    "        # Normalize the activation\n",
    "        # k prevents the division by 0 which can be caused by ReLu\n",
    "        x = tf.divide(x, tf.pow((self.k + self.alpha*sum_adjacent), tf.cast(self.beta, tf.float32)))\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # The LRN layer does not change the output shape\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Build the model\n",
    "\n",
    "# L1: CONV(11 x 11 x 3 x 96, stride = 4) -> ReLU\n",
    "# L2: CONV(5 x 5 x 96 x 256) -> ReLU -> Response normalization -> MaxPooling(3 x 3, stride = 2)\n",
    "# L3: CONV(3 x 3 x 256 x 384) -> ReLU -> Response normalization -> MaxPooling(3 x 3, stride = 2)\n",
    "# L4: CONV(3 x 3 x 384 x 384) -> ReLU \n",
    "# L5: CONV(3 x 3 x 384 x 256) -> ReLU -> MaxPooling(3 x 3, stride = 2)\n",
    "\n",
    "# L6: FC(4096) -> ReLU\n",
    "# L7: FC(4096) -> ReLU -> Flatten\n",
    "# L8: FC(1000) -> Softmax\n",
    "\n",
    "# Instanciate a LRN_layer\n",
    "#lrn_layer = LRN_layer()\n",
    "#print(lrn_layer(tf.ones((10, 3, 5, 5))))\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "# Layer 1:\n",
    "model.add(layers.Conv2D(96, (11, 11), strides = 4, activation = 'relu', input_shape = (227, 227, 3)))\n",
    "\n",
    "# Layer 2:\n",
    "model.add(layers.Conv2D(256, (5, 5), activation = 'relu'))\n",
    "model.add(LRN_layer())\n",
    "model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=2))\n",
    "\n",
    "# Layer 3:\n",
    "model.add(layers.Conv2D(384, (3, 3), activation = 'relu'))\n",
    "model.add(LRN_layer())\n",
    "model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=2))\n",
    "\n",
    "# Layer 4:\n",
    "model.add(layers.Conv2D(384, (3, 3), activation = 'relu'))\n",
    "\n",
    "# Layer 5:\n",
    "model.add(layers.Conv2D(256, (3, 3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=2))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "# Layer 6:\n",
    "model.add(layers.Dense(4096, activation = 'relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Layer 7:\n",
    "model.add(layers.Dense(4096, activation = 'relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Layer 8:\n",
    "model.add(layers.Dense(1000, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Training time vs Test time (average cropped images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
